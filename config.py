# MODEL HYPER PARAMETERS #
VOCAB_SIZE = 65 # this is only because we are using the char level tokenization from the datset
BLOCK_SIZE = 8
MODEL_DIMENSION = 64
NUM_HEADS = 4
NUM_TRANSFORMER_BLOCKS = 6

# Training Hyperparameters #
LEARNING_RATE = 0.0001
BATCH_SIZE = 4
ITERATIONS = 10000
    