# Transformer from Scratch
This is an educational codebase for an Autoregressive Language Model trained on Shakespeare text. The Language Model is powered by a decoder-stacked Transformer, which I implemented from scratch in PyTorch.  

This project is ongoing and I hope to implement a more advanced Tokenizer using the Byte-Pair Encoding Algorithm, train the model on different datasets, as well as add an encoder stack to apply the transformer architecture to different tasks such as Machine Translation.

Inspriation for project: Andrej Karpathy